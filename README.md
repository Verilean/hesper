# Hesper

**Write GPU programs in Lean 4. Prove them correct. Run on WebGPU.**

Hesper is a verified GPU programming framework that brings the power of formal verification to GPU computing. Write type-safe shaders, execute tensor operations, and build graphics applicationsâ€”all in Lean 4.

```lean
import Hesper.WGSL.DSL

-- Type-safe shader expressions with compile-time verification
let x : Exp (.scalar .f32) := var "x"
let y : Exp (.scalar .f32) := var "y"
let result := sqrt (x * x + y * y)  -- Generates: sqrt(x * x + y * y)

-- Cannot mix types (compile error!)
-- let wrong := x + (var "i" : Exp (.scalar .i32))  âœ— Type error!
```

## Why Hesper?

Modern GPU programming lacks safety guarantees. Hesper provides:

- **Type Safety**: Shaders are type-checked at compile time, preventing type mismatches
- **Formal Verification**: Prove correctness properties about your GPU programs
- **WebGPU Backend**: Cross-platform GPU access via Dawn (Metal, Vulkan, D3D12)
- **Lean Integration**: Use Lean's powerful theorem proving alongside GPU computation
- **Multi-GPU Support**: Select and coordinate across multiple GPU adapters

## Quick Start

### Prerequisites

- **Lean 4** (latest version recommended)
- **CMake** 3.16+
- **C++17** compiler (Clang/GCC)
- **Platform**: macOS (Metal), Linux (Vulkan), or Windows (D3D12/Vulkan)

### Installation

```bash
# Clone the repository
git clone https://github.com/Verilean/hesper.git
cd hesper

# Build native dependencies (this will take a while on first build)
lake run buildNative

# Build and run a demo
lake build dsl-basics
./.lake/build/bin/dsl-basics
```

### Your First Hesper Program

Create `MyFirst.lean`:

```lean
import Hesper
import Hesper.WebGPU.Device

def main : IO Unit := do
  -- Initialize WebGPU
  Hesper.init

  -- Get a GPU device
  let device â† Hesper.WebGPU.getDevice

  IO.println "âœ“ GPU ready!"
```

Build and run:

```bash
lake build myfirst
./.lake/build/bin/myfirst
```

## Features

### ğŸš€ Multi-Precision SIMD CPU Backend

Hardware-accelerated CPU operations with multi-precision support:

```lean
import Hesper.Simd
import Hesper.Float32
import Hesper.Float16

-- Float64 (8 bytes): Native Lean Float, NEON 2/op, AVX2 4/op
let a64 := FloatArray.mk #[1.0, 2.0, 3.0, 4.0]
let c64 := Hesper.Simd.simdAdd a64 b64

-- Float32 (4 bytes): 2x memory savings, NEON 4/op, AVX2 8/op
let a32 := Float32.fromFloatArray a64
let c32 := Float32.simdAdd a32 b32

-- Float16 (2 bytes): 4x memory savings, NEON 8/op, AVX2+F16C 8/op
-- Requires ARMv8.2-A FP16 or x86_64 F16C - returns error if unavailable
let hasFP16 â† Float16.hasHardwareSupport
if hasFP16 then
  let a16 â† Float16.fromFloatArray a64
  let c16 â† Float16.simdAdd a16 b16
```

**Architecture Detection:**
- ARM64: NEON (default) + optional FP16 vector arithmetic (ARMv8.2-A+)
- x86_64: AVX2 + F16C extension (Ivy Bridge+)
- Optional OpenMP multithreading support

**Zero-Conversion Architecture:**
All operations work directly on raw `ByteArray` with no automatic type conversions. Conversions are explicit only when needed.

### ğŸ¯ Type-Safe Shader DSL

Write WGSL shaders with Lean's type system guaranteeing correctness:

```lean
import Hesper.WGSL.DSL

-- Expressions are typed and checked at compile time
let x : Exp (.scalar .f32) := var "x"
let y : Exp (.scalar .f32) := var "y"

-- Arithmetic operators work naturally
let distance := sqrt (x * x + y * y)

-- Built-in functions
let clamped := Exp.clamp x (lit 0.0) (lit 1.0)
let power := Exp.pow x (lit 2.0)

-- Generate WGSL code
IO.println distance.toWGSL  -- Output: sqrt((x * x) + (y * y))
```

### âš™ï¸ GPU Computation & SIMD CPU Backend

Execute compute shaders on GPU or leverage SIMD acceleration on CPU:

```lean
import Hesper.Compute
import Hesper.Simd

-- Matrix multiplication on GPU
let A : Matrix 1024 1024 := ...
let B : Matrix 1024 1024 := ...

-- Runs on GPU automatically
let C â† matmul A B

-- CPU SIMD operations with multi-precision support
let a := FloatArray.mk #[1.0, 2.0, 3.0, 4.0]
let b := FloatArray.mk #[5.0, 6.0, 7.0, 8.0]
let c := Hesper.Simd.simdAdd a b  -- NEON/AVX2 acceleration

-- Float32 (2x memory savings)
let a32 := Float32.fromFloatArray a
let c32 := Float32.simdAdd a32 b32

-- Float16 (4x memory savings, hardware-accelerated)
let a16 â† Float16.fromFloatArray a
let c16 â† Float16.simdAdd a16 b16

-- Neural network layers with automatic differentiation
let conv â† Conv2D.create inputChannels outputChannels kernelSize
let output â† conv.forward input
```

### ğŸ® Graphics & Windowing

Build interactive graphics applications with GLFW integration:

```lean
import Hesper.GLFW

def main : IO Unit := do
  Hesper.init

  withGLFW do
    let window â† createWindow 800 600 "Hesper Graphics"
    let device â† Hesper.WebGPU.getDevice
    let surface â† createSurface device window

    -- Render loop
    gameLoop window surface
```

### ğŸ”Œ Multi-GPU Support

Enumerate and select GPUs in multi-GPU systems:

```lean
import Hesper.WebGPU.Device

-- List all available GPUs
Hesper.WebGPU.listAdapters

-- Select specific GPU
let device0 â† getDeviceByIndex 0  -- First GPU
let device1 â† getDeviceByIndex 1  -- Second GPU

-- Get adapter information
let info â† getAdapterInfo 0
IO.println s!"GPU: {info.name} (Backend: {info.backendType})"
```

## Examples

### WebGPU Tetris

A full Tetris implementation using GLFW and WebGPU, demonstrating:
- Dynamic shader generation
- Real-time rendering
- Input handling
- Game state management

```bash
lake build tetris
./.lake/build/bin/tetris
```

**Controls**: A/D (move), S (drop), Space (rotate), ESC (exit)

### Matrix Multiplication

High-performance matrix multiplication with subgroup optimizations:

```bash
lake build matmul-demo
./.lake/build/bin/matmul-demo
```

Demonstrates:
- GPU buffer management
- Compute shader execution
- Performance profiling
- Result verification

### SIMD CPU Backend

Multi-precision SIMD operations with hardware acceleration:

```bash
# Run multi-precision test (Float64/Float32/Float16)
lake script run buildSimd
lake build multi-precision
./.lake/build/bin/multi-precision

# Run SIMD benchmarks
lake build simd-bench
./.lake/build/bin/simd-bench
```

Output:
```
Backend: NEON (ARM64) - F64: 2/op, F32: 4/op, FP16

â”€â”€â”€ Float64 (8 bytes/element) â”€â”€â”€
Result: #[6.0, 8.0, 10.0, 12.0] âœ“

â”€â”€â”€ Float32 (4 bytes/element) â”€â”€â”€
Result: Float32[4]: [6.0, 8.0, 10.0, 12.0] âœ“

â”€â”€â”€ Float16 (2 bytes/element) â”€â”€â”€
FP16 hardware detected!
Result: Float16[4]: [6.0, 8.0, 10.0, 12.0] âœ“
```

### Multi-GPU Demo

Enumerate GPUs and create devices from specific adapters:

```bash
lake build multigpu
./.lake/build/bin/multigpu
```

Output:
```
Found 2 GPU adapter(s):
  [0] NVIDIA GeForce RTX 3080 (Backend: Vulkan)
  [1] Intel UHD Graphics 630 (Backend: Vulkan)
âœ“ Device created from GPU 0
```

### Neural Network Training

Automatic differentiation and gradient descent on GPU:

```bash
lake build nn-gpu-demo
./.lake/build/bin/nn-gpu-demo
```

Features:
- Conv2D layers with verified gradients
- Backpropagation on GPU
- Real-time training visualization

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Lean 4 Code                               â”‚
â”‚  â€¢ Type-safe shader DSL                                      â”‚
â”‚  â€¢ Tensor operations                                         â”‚
â”‚  â€¢ Formal proofs                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WGSL Code Generation                            â”‚
â”‚  Exp (.scalar .f32) â†’ WGSL shader source                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Lean FFI (C++ Bridge)                           â”‚
â”‚  â€¢ lean_hesper_* functions                                   â”‚
â”‚  â€¢ Resource management via Lean.External                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Google Dawn (WebGPU Native)                     â”‚
â”‚  â€¢ Metal (macOS)                                             â”‚
â”‚  â€¢ Vulkan (Linux/Windows)                                    â”‚
â”‚  â€¢ D3D12 (Windows)                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture Layers

1. **DSL Layer**: Type-safe WGSL expression builder with dependent types
2. **Tensor Layer**: High-level operations (matmul, conv2d, pooling)
3. **Compute Layer**: Shader compilation, buffer management, execution
4. **WebGPU Layer**: FFI bindings to Dawn native implementation
5. **Backend Layer**: Platform-specific GPU drivers (Metal/Vulkan/D3D12)

## Project Structure

```
Hesper/
â”œâ”€â”€ Hesper/
â”‚   â”œâ”€â”€ WGSL/          # Type-safe shader DSL
â”‚   â”‚   â”œâ”€â”€ Types.lean      # WGSL type system
â”‚   â”‚   â”œâ”€â”€ Exp.lean        # Expression AST
â”‚   â”‚   â””â”€â”€ DSL.lean        # User-facing DSL
â”‚   â”œâ”€â”€ WebGPU/        # WebGPU bindings
â”‚   â”‚   â”œâ”€â”€ Device.lean     # GPU device management
â”‚   â”‚   â”œâ”€â”€ Buffer.lean     # GPU buffers
â”‚   â”‚   â”œâ”€â”€ Shader.lean     # Shader modules
â”‚   â”‚   â”œâ”€â”€ Pipeline.lean   # Compute/render pipelines
â”‚   â”‚   â””â”€â”€ Errors.lean     # Comprehensive error handling
â”‚   â”œâ”€â”€ Tensor/        # Tensor operations
â”‚   â”‚   â””â”€â”€ MatMul.lean     # Matrix multiplication
â”‚   â”œâ”€â”€ NN/            # Neural network layers
â”‚   â”‚   â””â”€â”€ Conv.lean       # Convolution layers
â”‚   â”œâ”€â”€ GLFW/          # Windowing and graphics
â”‚   â”‚   â””â”€â”€ GLFW.lean       # GLFW bindings
â”‚   â”œâ”€â”€ Simd.lean      # SIMD Float64 operations
â”‚   â”œâ”€â”€ Float32.lean   # SIMD Float32 operations
â”‚   â”œâ”€â”€ Float16.lean   # SIMD Float16 operations
â”‚   â””â”€â”€ Compute.lean   # High-level compute API
â”œâ”€â”€ Examples/          # Example programs
â”‚   â”œâ”€â”€ Tetris.lean         # Full game demo
â”‚   â”œâ”€â”€ MultiGPU.lean       # Multi-GPU support
â”‚   â”œâ”€â”€ DSLBasics.lean      # DSL tutorial
â”‚   â””â”€â”€ ...
â”œâ”€â”€ native/            # C++ WebGPU bridge
â”‚   â”œâ”€â”€ bridge.cpp          # FFI implementation
â”‚   â””â”€â”€ CMakeLists.txt      # Build configuration
â”œâ”€â”€ c_src/             # SIMD CPU backend
â”‚   â””â”€â”€ simd_ops.cpp        # NEON/AVX2 implementations
â”œâ”€â”€ Tests/             # Comprehensive test suite
â”‚   â”œâ”€â”€ ErrorTests.lean     # Error handling tests
â”‚   â”œâ”€â”€ ShaderTests.lean    # Shader monad tests
â”‚   â””â”€â”€ ...
â””â”€â”€ lakefile.lean      # Lake build script
```

## Roadmap

**Current Status**: Early Development (Alpha)

Completed:
- [x] WebGPU device initialization via Dawn
- [x] Type-safe WGSL DSL
- [x] Compute shader execution
- [x] Buffer management (GPU â†” CPU)
- [x] GLFW windowing integration
- [x] Multi-GPU adapter enumeration
- [x] Basic matrix operations
- [x] Convolution layers
- [x] Automatic differentiation
- [x] **Multi-precision SIMD CPU backend (Float64/Float32/Float16)**
- [x] **Architecture detection (NEON/AVX2/F16C)**
- [x] **Comprehensive error handling with structured error types**
- [x] **Complete test suite (error handling, shader monad)**

In Progress:
- [ ] Comprehensive tensor operation library
- [ ] Neural network training framework
- [ ] Performance optimization (subgroup operations)
- [ ] Verification of GPU kernel correctness

Future:
- [ ] Formal proofs of numerical stability
- [ ] Compiler optimizations for shader generation
- [ ] Distributed multi-GPU training
- [ ] Integration with Lean's tactic framework
- [ ] Ray tracing support
- [ ] Multi-precision GPU tensors (Float16 for memory bandwidth optimization)

## Contributing

Hesper is part of the **Verilean** organization's effort to bring verified computing to GPUs.

- **Report Issues**: [GitHub Issues](https://github.com/Verilean/hesper/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Verilean/hesper/discussions)
- **Sister Project**: [Sparkle HDL](https://github.com/Verilean/sparkle) - Verified hardware design in Lean 4

## Author

**Junji Hashimoto**

Twitter/X: [@junjihashimoto3](https://twitter.com/junjihashimoto3)

## License

Apache License 2.0 - see LICENSE file for details

## Acknowledgments

- **Google Dawn** for the WebGPU native implementation
- **Lean 4** for the foundation of verified programming
- **WebGPU Working Group** for the standard
- **Verilean Community** for support and contributions

---

*Write GPU code that's not just fastâ€”make it correct by construction.*
